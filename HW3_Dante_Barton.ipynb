{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dantebarton/collabNotebook/blob/main/HW3_Dante_Barton.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Digit Classification with the MNIST Dataset\n",
        "\n",
        "\n",
        "Objective:\n",
        "The primary goal of this assignment is to provide hands-on experience with loading datasets, preparing data for training, and using a  neural network architecture to classify handwritten digits from the MNIST dataset. This assignment aims to deepen understanding of neural networks and familiarize students with practical aspects of training models using PyTorch.\n",
        "\n",
        "Setup and Requirements Installation:"
      ],
      "metadata": {
        "id": "uP6qIP_Wyv-s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch torchvision"
      ],
      "metadata": {
        "id": "BB2gEaviw4IV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abe23fab-7bb4-4b17-be52-984dc86aec24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.2.1+cu121)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.17.1+cu121)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.3)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch) (4.10.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-runtime-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m29.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cuda-cupti-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cudnn-cu12==8.9.2.26 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m916.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cublas-cu12==12.1.3.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cufft-cu12==11.0.2.54 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-curand-cu12==10.3.2.106 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusolver-cu12==11.4.5.107 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-cusparse-cu12==12.1.0.106 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nccl-cu12==2.19.3 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.19.3-py3-none-manylinux1_x86_64.whl (166.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m99.1/99.1 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.2.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.25.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.5)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12\n",
            "Successfully installed nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.19.3 nvidia-nvjitlink-cu12-12.4.127 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Load the MNIST Dataset\n",
        "First, let's import the necessary libraries and load the MNIST dataset.\n",
        "\n"
      ],
      "metadata": {
        "id": "cXKGEzPf0q_p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#todo: based on what you learned in assignment 2, load train and test datasets\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# Define a transform to normalize the data\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                transforms.Normalize((0.5,), (0.5,))])\n",
        "\n",
        "# Download and load the training data\n",
        "trainset = datasets.MNIST('~/.pytorch/MNIST_data/', download=True, train=True, transform=transform)\n",
        "testloader = torch.utils.data.DataLoader(trainset, batch_size=64, shuffle=True)\n",
        "\n",
        "# Get one batch of training images and labels\n",
        "# images, labels = next(iter(trainloader))\n",
        "# print(\"Shape of images:\", images.shape)\n",
        "# print(\"Shape of labels:\", labels.shape)"
      ],
      "metadata": {
        "id": "U4pHf3BLzG-n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Define the Model Architecture\n",
        "Here is the neural network model architecture ."
      ],
      "metadata": {
        "id": "-SipTNTb1QhL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Original Model Architecture\n",
        "# class Net(nn.Module):\n",
        "#     def __init__(self):\n",
        "#         super(Net, self).__init__()\n",
        "#         self.flatten = nn.Flatten()\n",
        "#         self.fc1 = nn.Linear(28*28, 128)\n",
        "#         self.dropout = nn.Dropout(0.2)\n",
        "#         self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "#     def forward(self, x):\n",
        "#         x = self.flatten(x)\n",
        "#         x = F.relu(self.fc1(x))\n",
        "#         x = self.dropout(x)\n",
        "#         x = self.fc2(x)\n",
        "#         return F.log_softmax(x, dim=1)\n",
        "\n",
        "# model = Net()\n",
        "\n",
        "#Modified Model Architecture#\n",
        "\n",
        "class Net(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Net, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28*28, 256)  # Increased number of neurons in the first hidden layer\n",
        "        self.fc2 = nn.Linear(256, 128)    # Decreased number of neurons in the second hidden layer\n",
        "        self.dropout1 = nn.Dropout(0.2)   # Added dropout layer after the first hidden layer\n",
        "        self.fc3 = nn.Linear(128, 64)     # Additional fully connected layer\n",
        "        self.dropout2 = nn.Dropout(0.2)   # Added dropout layer after the additional fully connected layer\n",
        "        self.fc4 = nn.Linear(64, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout1(x)\n",
        "        x = F.relu(self.fc2(x))\n",
        "        x = self.dropout2(x)\n",
        "        x = F.relu(self.fc3(x))          # Applying ReLU activation after the additional fully connected layer\n",
        "        x = self.fc4(x)\n",
        "        return F.log_softmax(x, dim=1)\n",
        "\n",
        "# Instantiate the modified model\n",
        "model = Net()\n",
        "\n"
      ],
      "metadata": {
        "id": "LzPjwjPgzV9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Train the Model\n",
        "Define the optimizer and loss function, and then train the model."
      ],
      "metadata": {
        "id": "PAqFgw9F1cZy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import optim\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.003)\n",
        "criterion = nn.NLLLoss()\n",
        "\n",
        "epochs = 5\n",
        "for e in range(epochs):\n",
        "    running_loss = 0\n",
        "    for images, labels in trainloader:\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        log_ps = model(images)\n",
        "        loss = criterion(log_ps, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "    else:\n",
        "        print(f\"Training loss: {running_loss/len(trainloader)}\")\n"
      ],
      "metadata": {
        "id": "KzD8U0IXzhJY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5937afce-5e30-4e82-ab9f-dcb6fd47ef11"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss: 0.42441764193525444\n",
            "Training loss: 0.2605543776369616\n",
            "Training loss: 0.22789491272604923\n",
            "Training loss: 0.20418292797927154\n",
            "Training loss: 0.19269687280670475\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Evaluate the Model\n",
        "Finally, evaluate the model's performance on the test dataset."
      ],
      "metadata": {
        "id": "A-cGZEbc1rrN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "correct = 0\n",
        "total = 0\n",
        "with torch.no_grad():\n",
        "    for images, labels in testloader:\n",
        "        log_ps = model(images)\n",
        "        _, predicted = torch.max(log_ps.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "print(f'Accuracy of the network on the 10000 test images: {100 * correct // total} %')\n"
      ],
      "metadata": {
        "id": "RjIoOnz1zlm0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "39ad97ad-81b8-49a3-e8fa-ea764fe03d74"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy of the network on the 10000 test images: 94 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Change the Number of Epochs\n",
        "# Modify the number of epochs in the training loop. Try training the model for more epochs (e.g., 10, 15) and fewer epochs (e.g., 1, 3). Observe how this affects the training loss and the accuracy on the test dataset.\n",
        "\n",
        "# How does increasing the number of epochs affect the model's performance and training time? explain\n",
        "\n",
        "#In my tests I tried training the model with the following epochs in the following order and received the results in the brackets: 5(94%), 10(95%), 15(95%), 50(96%), 3(96%), 1(97%). When I increased the number of epochs the model's performance increased since it multiples tries to learn from the dataset\n",
        "#and adjust accordingly. This however lead to longer training time for example with epoch set to 50 it took 14 minutes to complete. I also realized that after training 50 before attempting lower number that the data set might've begun memorizing patterns in the dataset and after a bit of research I learned this is called\n",
        "#overfitting, where the model learns to memorize the training data instead of generalizing well to unseen data."
      ],
      "metadata": {
        "id": "sZZimflv0F6S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Modify the Model Architecture\n",
        "# Change the architecture of the neural network. Here are a few suggestions:\n",
        "# Add another fully connected layer with ReLU activation and observe the impact.\n",
        "# Increase or decrease the number of neurons in the hidden layers.\n",
        "# Introduce dropout in additional places or modify the dropout rate.\n",
        "\n",
        "#How does each modification affect the model's accuracy and training behavior? explain.\n",
        "\n",
        "#After researching each change and understanding the impact on the model's accuracy and training behavior I have found the following to be true:\n",
        "# 1. Add another fully connected layer with ReLU activation:\n",
        "# This modification can potentially allow the model to learn more complex representations of the data by introducing additional nonlinear transformations. However, adding more parameters may also increase the risk of overfitting, especially if the dataset is not sufficiently large. The impact on accuracy would depend on whether the added complexity helps capture important patterns in the data.\n",
        "\n",
        "# 2. Increase or decrease the number of neurons in the hidden layers:\n",
        "# Increasing the number of neurons can provide the model with more capacity to learn complex relationships in the data. This can lead to improved accuracy, especially if the previous architecture was underfitting. However, increasing the number of parameters also increases the risk of overfitting. Decreasing the number of neurons may lead to a simpler model that generalizes better but might sacrifice some accuracy.\n",
        "\n",
        "# 3. Introduce dropout in additional places or modify the dropout rate:\n",
        "# Dropout is a regularization technique used to prevent overfitting by randomly dropping units (along with their connections) during training. Increasing the dropout rate or introducing dropout in additional places can help in preventing overfitting, especially in deeper networks or with larger datasets. However, too much dropout can lead to underfitting, where the model fails to capture important patterns in the data.\n",
        "\n",
        "# Can you achieve higher accuracy with a more complex or deeper network? explain\n",
        "\n",
        "#Achieving higher accuracy with a more complex or deeper network is possible, but it requires careful consideration of the dataset size and the complexity.\n"
      ],
      "metadata": {
        "id": "BR3htPGu2WQS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example Changes:\n",
        "\n",
        "class ModifiedNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ModifiedNet, self).__init__()\n",
        "        self.flatten = nn.Flatten()\n",
        "        self.fc1 = nn.Linear(28*28, 256)  # Increased number of neurons\n",
        "        self.fc2 = nn.Linear(256, 128)    # Additional layer\n",
        "        self.dropout = nn.Dropout(0.3)    # Adjusted dropout rate\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.flatten(x)\n",
        "        x = F.relu(self.fc1(x))\n",
        "        x = self.dropout(x)\n",
        "        x = F.relu(self.fc2(x))           # Additional ReLU activation\n",
        "        x = self.dropout(x)               # Additional dropout\n",
        "        x = self.fc3(x)\n",
        "        return F.log_softmax(x, dim=1)\n"
      ],
      "metadata": {
        "id": "BqKbUmdD2jV4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Visualize the Predictions\n"
      ],
      "metadata": {
        "id": "qgQ0qi7Q4tHK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Select a few images from the test set and visualize their predicted labels along with the true labels.\n",
        "# Use Matplotlib to display the images and predictions.\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Function to visualize images and their predicted/true labels\n",
        "def visualize_predictions(images, predicted_labels, true_labels, class_names):\n",
        "    fig, axes = plt.subplots(nrows=1, ncols=len(images), figsize=(12, 4))\n",
        "    for i, ax in enumerate(axes):\n",
        "        ax.imshow(images[i].squeeze(), cmap='gray')\n",
        "        ax.set_title(f\"Predicted: {class_names[predicted_labels[i]]}\\nTrue: {class_names[true_labels[i]]}\")\n",
        "        ax.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "# Select a few images and their true labels from the test set\n",
        "num_images_to_visualize = 6\n",
        "images_to_visualize = []\n",
        "true_labels_to_visualize = []\n",
        "predicted_labels = []\n",
        "\n",
        "for i, (images, labels) in enumerate(testloader):\n",
        "    if i == num_images_to_visualize:\n",
        "        break\n",
        "    images_to_visualize.append(images[0])  # Assuming batch size is 1\n",
        "    true_labels_to_visualize.append(labels[0].item())\n",
        "\n",
        "# Convert images and true labels to tensors\n",
        "images_to_visualize = torch.stack(images_to_visualize)\n",
        "true_labels_to_visualize = torch.tensor(true_labels_to_visualize)\n",
        "\n",
        "# Use the trained model to predict labels for the selected images\n",
        "with torch.no_grad():\n",
        "    model.eval()\n",
        "    outputs = model(images_to_visualize)\n",
        "    _, predicted_labels = torch.max(outputs, 1)\n",
        "\n",
        "# Define class names\n",
        "class_names = [str(i) for i in range(10)]\n",
        "\n",
        "# Visualize the predictions\n",
        "visualize_predictions(images_to_visualize, predicted_labels, true_labels_to_visualize, class_names)\n"
      ],
      "metadata": {
        "id": "GQVujieX252f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 212
        },
        "outputId": "96d898dc-593a-4d19-dffd-3adc9281b198"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x400 with 6 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    }
  ]
}